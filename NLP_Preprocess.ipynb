{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKdYER8+8XREfJ/26zp/Z/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savasturk/NLP-Dogal_Dil_Isleme/blob/main/NLP_Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Reuters corpus örneği\n",
        "nltk.download('reuters')\n",
        "# Reuters corpus'tan bir örnek belge al\n",
        "document_id = 'test/14826'\n",
        "document = reuters.raw(document_id)\n",
        "\n",
        "print(document[0:259])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWppB97kACSB",
        "outputId": "dc4b81df-790d-42f7-9b8b-a9f1f1bd8ec2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
            "  Mounting trade friction between the\n",
            "  U.S. And Japan has raised fears among many of Asia's exporting\n",
            "  nations that the row could inflict far-reaching economic\n",
            "  damage, businessmen and officials said.\n",
            "      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Örnek bir metin\n",
        "text1 = \"Hello, how are you? I hope you are doing well. Break off, Turn around. My cherry pie fell apart when I tried to cut it. Doing swimming. better. The weather today is much better than yesterday. \""
      ],
      "metadata": {
        "id": "1stTP5sYrhfF"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"Mounting trade friction between the U.S. And Japan has raised fears among many of Asia's exporting nations that the row could inflict far-reaching economicdamage, businessmen and officials said.\""
      ],
      "metadata": {
        "id": "vgyCvf96Anzv"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text1 + text2"
      ],
      "metadata": {
        "id": "LhonnuxeAze0"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Metni cümlelere ayırma\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LHWO9burosQ",
        "outputId": "a249af6e-aacd-40ff-fc83-db3651c8ab19"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you?\n",
            "I hope you are doing well.\n",
            "Break off, Turn around.\n",
            "My cherry pie fell apart when I tried to cut it.\n",
            "Doing swimming.\n",
            "better.\n",
            "The weather today is much better than yesterday.\n",
            "Mounting trade friction between the U.S. And Japan has raised fears among many of Asia's exporting nations that the row could inflict far-reaching economicdamage, businessmen and officials said.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Metni kelimelere ayırma\n",
        "words = word_tokenize(text)\n",
        "\n",
        "for word in words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLF4DGsYstWE",
        "outputId": "53b2767f-40a3-4a5e-decd-5ed9f516ac0a"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            "how\n",
            "are\n",
            "you\n",
            "?\n",
            "I\n",
            "hope\n",
            "you\n",
            "are\n",
            "doing\n",
            "well\n",
            ".\n",
            "Break\n",
            "off\n",
            ",\n",
            "Turn\n",
            "around\n",
            ".\n",
            "My\n",
            "cherry\n",
            "pie\n",
            "fell\n",
            "apart\n",
            "when\n",
            "I\n",
            "tried\n",
            "to\n",
            "cut\n",
            "it\n",
            ".\n",
            "Doing\n",
            "swimming\n",
            ".\n",
            "better\n",
            ".\n",
            "The\n",
            "weather\n",
            "today\n",
            "is\n",
            "much\n",
            "better\n",
            "than\n",
            "yesterday\n",
            ".\n",
            "Mounting\n",
            "trade\n",
            "friction\n",
            "between\n",
            "the\n",
            "U.S.\n",
            "And\n",
            "Japan\n",
            "has\n",
            "raised\n",
            "fears\n",
            "among\n",
            "many\n",
            "of\n",
            "Asia\n",
            "'s\n",
            "exporting\n",
            "nations\n",
            "that\n",
            "the\n",
            "row\n",
            "could\n",
            "inflict\n",
            "far-reaching\n",
            "economicdamage\n",
            ",\n",
            "businessmen\n",
            "and\n",
            "officials\n",
            "said\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Lemmatizer oluştur\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sözcükleri indirge\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "for lemmatized_word in lemmatized_words:\n",
        "    print(lemmatized_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjT9vOsEPcrv",
        "outputId": "091370c5-8e32-4aba-9179-30f2d80d63e7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            "how\n",
            "be\n",
            "you\n",
            "?\n",
            "I\n",
            "hope\n",
            "you\n",
            "be\n",
            "do\n",
            "well\n",
            ".\n",
            "Break\n",
            "off\n",
            ",\n",
            "Turn\n",
            "around\n",
            ".\n",
            "My\n",
            "cherry\n",
            "pie\n",
            "fell\n",
            "apart\n",
            "when\n",
            "I\n",
            "try\n",
            "to\n",
            "cut\n",
            "it\n",
            ".\n",
            "Doing\n",
            "swim\n",
            ".\n",
            "well\n",
            ".\n",
            "The\n",
            "weather\n",
            "today\n",
            "be\n",
            "much\n",
            "good\n",
            "than\n",
            "yesterday\n",
            ".\n",
            "Mounting\n",
            "trade\n",
            "friction\n",
            "between\n",
            "the\n",
            "U.S.\n",
            "And\n",
            "Japan\n",
            "have\n",
            "raise\n",
            "fear\n",
            "among\n",
            "many\n",
            "of\n",
            "Asia\n",
            "'s\n",
            "export\n",
            "nation\n",
            "that\n",
            "the\n",
            "row\n",
            "could\n",
            "inflict\n",
            "far-reaching\n",
            "economicdamage\n",
            ",\n",
            "businessmen\n",
            "and\n",
            "official\n",
            "say\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Porter Stemmer'ı oluşturma\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Kök indirgeme işlemi\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "for stemmed_word in stemmed_words:\n",
        "    print(stemmed_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQZzyMxzsw5O",
        "outputId": "667db914-5bd0-4592-f265-41f5a8840a47"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            ",\n",
            "how\n",
            "are\n",
            "you\n",
            "?\n",
            "i\n",
            "hope\n",
            "you\n",
            "are\n",
            "do\n",
            "well\n",
            ".\n",
            "break\n",
            "off\n",
            ",\n",
            "turn\n",
            "around\n",
            ".\n",
            "my\n",
            "cherri\n",
            "pie\n",
            "fell\n",
            "apart\n",
            "when\n",
            "i\n",
            "tri\n",
            "to\n",
            "cut\n",
            "it\n",
            ".\n",
            "do\n",
            "swim\n",
            ".\n",
            "better\n",
            ".\n",
            "the\n",
            "weather\n",
            "today\n",
            "is\n",
            "much\n",
            "better\n",
            "than\n",
            "yesterday\n",
            ".\n",
            "mount\n",
            "trade\n",
            "friction\n",
            "between\n",
            "the\n",
            "u.s.\n",
            "and\n",
            "japan\n",
            "ha\n",
            "rais\n",
            "fear\n",
            "among\n",
            "mani\n",
            "of\n",
            "asia\n",
            "'s\n",
            "export\n",
            "nation\n",
            "that\n",
            "the\n",
            "row\n",
            "could\n",
            "inflict\n",
            "far-reach\n",
            "economicdamag\n",
            ",\n",
            "businessmen\n",
            "and\n",
            "offici\n",
            "said\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# İngilizce stopwords listesini yükleme\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGeFZydetIrH",
        "outputId": "1e65495c-969b-4b16-eccd-0aa43eb4b46b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOooKbEx_Z3y",
        "outputId": "1cadcf93-fe35-4880-ebe2-a36aed82e229"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'o', 'that', 'most', 'under', 'couldn', 'his', 'while', \"shan't\", 'himself', 'before', 'shan', 'nor', 'you', \"you're\", 'this', \"weren't\", 'm', 'in', 'was', \"should've\", 'have', 'what', 'your', \"you'd\", 'itself', 'theirs', 'those', 'over', 'won', 'hers', 'against', 'but', 'where', 'do', 'll', 'ours', 'isn', 'shouldn', 'which', 'them', 'did', 'up', 'some', \"wouldn't\", \"hadn't\", \"mustn't\", \"you'll\", 'the', 'after', \"you've\", 'out', 'why', 'yourselves', 'here', 'haven', 'if', 'both', 'a', 'during', 'more', \"didn't\", 'are', \"she's\", 'doing', 'into', 'weren', 'me', \"haven't\", 'don', 'has', 'ain', 'as', 'how', 'she', 'yourself', 'on', \"mightn't\", \"aren't\", 'had', 'of', 're', 'my', 'then', \"won't\", 'wouldn', 'd', 'hadn', 'him', 'can', 'at', 'they', \"don't\", 'ma', 'been', 'yours', 'be', 'mustn', 'he', 'for', 'further', 't', 'to', 'such', 'ourselves', 'it', 'once', 'is', 'few', 'am', 'having', 'doesn', 'being', 'and', 's', 'about', 'down', \"shouldn't\", 've', \"needn't\", 'because', 'between', 'who', \"couldn't\", 'only', 'mightn', 'own', \"that'll\", 'we', 'below', 'by', 'until', 'there', 'other', 'through', 'than', 'these', 'now', \"wasn't\", \"it's\", 'themselves', 'when', \"hasn't\", 'wasn', 'does', 'y', 'whom', 'any', 'will', 'all', 'aren', 'hasn', 'again', 'no', 'should', 'needn', 'were', 'so', 'above', 'myself', 'an', 'not', \"isn't\", 'too', \"doesn't\", 'each', 'same', 'didn', 'very', 'i', 'herself', 'or', 'its', 'her', 'their', 'with', 'off', 'just', 'our', 'from'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Etkisiz sözcükleri çıkarma\n",
        "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]\n",
        "\n",
        "for filtered_word in filtered_words:\n",
        "    print(filtered_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTyFBVzetKOE",
        "outputId": "2058472d-084c-42b3-a892-412fc364698a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            "?\n",
            "hope\n",
            "well\n",
            ".\n",
            "Break\n",
            ",\n",
            "Turn\n",
            "around\n",
            ".\n",
            "cherry\n",
            "pie\n",
            "fell\n",
            "apart\n",
            "try\n",
            "cut\n",
            ".\n",
            "swim\n",
            ".\n",
            "well\n",
            ".\n",
            "weather\n",
            "today\n",
            "much\n",
            "good\n",
            "yesterday\n",
            ".\n",
            "Mounting\n",
            "trade\n",
            "friction\n",
            "U.S.\n",
            "Japan\n",
            "raise\n",
            "fear\n",
            "among\n",
            "many\n",
            "Asia\n",
            "'s\n",
            "export\n",
            "nation\n",
            "row\n",
            "could\n",
            "inflict\n",
            "far-reaching\n",
            "economicdamage\n",
            ",\n",
            "businessmen\n",
            "official\n",
            "say\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# Noktalama işaretleri listesi\n",
        "punctuation_list = set(string.punctuation)\n",
        "\n",
        "# Noktalama işaretlerini kaldırma\n",
        "cleaned_words = [word for word in filtered_words if word not in punctuation_list]\n",
        "\n",
        "for cleaned_word in cleaned_words:\n",
        "    print(cleaned_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0L2V_TCtjMe",
        "outputId": "c448a770-11d2-4c7f-aad4-b82ab45a9790"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "hope\n",
            "well\n",
            "Break\n",
            "Turn\n",
            "around\n",
            "cherry\n",
            "pie\n",
            "fell\n",
            "apart\n",
            "try\n",
            "cut\n",
            "swim\n",
            "well\n",
            "weather\n",
            "today\n",
            "much\n",
            "good\n",
            "yesterday\n",
            "Mounting\n",
            "trade\n",
            "friction\n",
            "U.S.\n",
            "Japan\n",
            "raise\n",
            "fear\n",
            "among\n",
            "many\n",
            "Asia\n",
            "'s\n",
            "export\n",
            "nation\n",
            "row\n",
            "could\n",
            "inflict\n",
            "far-reaching\n",
            "economicdamage\n",
            "businessmen\n",
            "official\n",
            "say\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Kelimelerin türlerini etiketleme\n",
        "word_tags = pos_tag(words)\n",
        "\n",
        "for word_tag, tag in word_tags:\n",
        "    print(f\"{get_wordnet_pos(tag)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FLaXOQ7tSr-",
        "outputId": "70552b9c-e521-4c0b-db62-1d190cdefe39"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "İsim\n",
            "Diğer\n",
            "Belirteç\n",
            "Fiil\n",
            "Zamir\n",
            "Diğer\n",
            "Zamir\n",
            "Fiil\n",
            "Zamir\n",
            "Fiil\n",
            "Fiil\n",
            "Zarf\n",
            "Diğer\n",
            "İsim\n",
            "Edat\n",
            "Diğer\n",
            "İsim\n",
            "Zarf\n",
            "Diğer\n",
            "İsim\n",
            "İsim\n",
            "İsim\n",
            "Fiil\n",
            "Zarf\n",
            "Belirteç\n",
            "Zamir\n",
            "Fiil\n",
            "to Belirteci\n",
            "Fiil\n",
            "Zamir\n",
            "Diğer\n",
            "Fiil\n",
            "Fiil\n",
            "Diğer\n",
            "Zarf\n",
            "Diğer\n",
            "Belirteç\n",
            "İsim\n",
            "İsim\n",
            "Fiil\n",
            "Zarf\n",
            "Sıfat\n",
            "Edat\n",
            "İsim\n",
            "Diğer\n",
            "Fiil\n",
            "İsim\n",
            "İsim\n",
            "Edat\n",
            "Belirteç\n",
            "İsim\n",
            "Bağlaç\n",
            "İsim\n",
            "Fiil\n",
            "Fiil\n",
            "İsim\n",
            "Edat\n",
            "Sıfat\n",
            "Edat\n",
            "İsim\n",
            "Zamir\n",
            "Fiil\n",
            "İsim\n",
            "Edat\n",
            "Belirteç\n",
            "İsim\n",
            "Belirli\n",
            "Fiil\n",
            "Sıfat\n",
            "İsim\n",
            "Diğer\n",
            "İsim\n",
            "Bağlaç\n",
            "İsim\n",
            "Fiil\n",
            "Diğer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('N'):\n",
        "        return 'İsim'\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'Fiil'\n",
        "    elif treebank_tag.startswith('J'):\n",
        "        return 'Sıfat'\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'Zarf'\n",
        "    elif treebank_tag.startswith('P'):\n",
        "        return 'Zamir'\n",
        "    elif treebank_tag.startswith('C'):\n",
        "        return 'Bağlaç'\n",
        "    elif treebank_tag.startswith('DT'):\n",
        "        return 'Belirteç'\n",
        "    elif treebank_tag.startswith('IN'):\n",
        "        return 'Edat'\n",
        "    elif treebank_tag.startswith('PRP'):\n",
        "        return 'Zamir'\n",
        "    elif treebank_tag.startswith('RB'):\n",
        "        return 'Zarf'\n",
        "    elif treebank_tag.startswith('JJ'):\n",
        "        return 'Sıfat'\n",
        "    elif treebank_tag.startswith('CC'):\n",
        "        return 'Bağlaç'\n",
        "    elif treebank_tag.startswith('CD'):\n",
        "        return 'Sayı'\n",
        "    elif treebank_tag.startswith('EX'):\n",
        "        return 'Orman Belirtec'\n",
        "    elif treebank_tag.startswith('FW'):\n",
        "        return 'Yabancı Kelime'\n",
        "    elif treebank_tag.startswith('LS'):\n",
        "        return 'Liste İşareti'\n",
        "    elif treebank_tag.startswith('MD'):\n",
        "        return 'Belirli'\n",
        "    elif treebank_tag.startswith('POS'):\n",
        "        return 'Belirleyici'\n",
        "    elif treebank_tag.startswith('SYM'):\n",
        "        return 'Sembol'\n",
        "    elif treebank_tag.startswith('TO'):\n",
        "        return 'to Belirteci'\n",
        "    elif treebank_tag.startswith('UH'):\n",
        "        return 'İnterjeksiyon'\n",
        "    elif treebank_tag.startswith('WDT'):\n",
        "        return 'Belirteç'\n",
        "    elif treebank_tag.startswith('WP'):\n",
        "        return 'Belirteç'\n",
        "    elif treebank_tag.startswith('WRB'):\n",
        "        return 'Belirteç'\n",
        "    else:\n",
        "        return 'Diğer'\n"
      ],
      "metadata": {
        "id": "PKxzPJJLdlEn"
      },
      "execution_count": 118,
      "outputs": []
    }
  ]
}